== TensorFlow Lite From Scratch for Embedded Linux

This is a developer oriented overview for bringing up TensorFlow Lite
using buildroot, an embedded linux system builder.

This will be part one of a series of running neural network inference
on embedded linux devices.  My primary interests lie in neural network
inference performance of existing models, and less on model selection
or training.

=== Step 0:  Hardware and other random prerequisites

I got this working with pretty minimal extras:

* Raspberry Pi 3
* Logitech Webcam
* 4GB SD card

It is completely reasonable to get embedded linux running on a
4GB SD card, where it can be a tight fit for other desktop oriented
distros.  I also have a Linux laptop from which I can build the buildroot
distro, as well as tensorflow lite.

=== Step 1:  Buildroot

Download a relatively recent release from the buildroot website
(https://buildroot.org/download.html), or
clone the git repository from github.

-----
$ wget https://buildroot.org/downloads/buildroot-2020.02.4.tar.gz
$ tar zxvf buildroot-2020.02.4.tar.gz
-----

I was able to take the stock
configuration for 64-bit raspberry pi 3 (`raspberrypi3_64_defconfig`)
for this exercise and get it to work with a couple of minor modifications.

The default `"120M"` size will be cramped to house the entire linux image
along with some of the models and additional software we will run, so
increase the size of the filesystem image at the outset.

First, change the root filesystem size with:
`BR2_TARGET_ROOTFS_EXT2_SIZE="2G"`

Second, let's provide a differentiated toolchain, using musl libc:

-----
BR2_TOOLCHAIN_BUILDROOT_VENDOR="kerikeri"
BR2_TOOLCHAIN_BUILDROOT_MUSL=y
BR2_TOOLCHAIN_BUILDROOT_LIBC="musl"
-----

Lastly, we will add SSH to allow for logging
into the board remotely:

`BR2_PACKAGE_OPENSSH=y`

Note that this configuration is not complete enough to enable wifi
on the board, though it is complete enough to connect over ethernet
via ssh.

With that, we're ready to build buildroot.

-----
$ cd buildroot-2020.02.4
<edit configs/raspberrypi3_64_defconfig as above>
$ make raspberrypi3_64_defconfig && make
<...wait...>
-----

We can now write the sdcard image directly to the sdcard.
This can be done as root with Linux's dd utility.
My sdcard reader/writer mounts the card on `/dev/sdb`
If there are partitions of the card that are already mounted (maybe as
`/dev/sdb1`, `/dev/sdb2`, etc.), you should unmount them before proceding.
Also, *make sure* that `/dev/sdb` is not some other media (hard-disk) that
should not get over-written.

-----
$ lsblk
...
sdb      8:16   1   3.7G  0 disk 
├─sdb1   8:17   1    32M  0 part /media/peter/B40E-45D2
└─sdb2   8:18   1   120M  0 part /media/peter/rootfs
$ umount /dev/sdb1 # if necessary
$ umount /dev/sdb2 # if necessary
$ ls -l output/images/sdcard.img
-rw-r--r-- 1 peter peter 2181038592 Aug  9 13:39 output/images/sdcard.img
$ sudo dd if=output/images/sdcard.img of=/dev/sdb bs=1M
[sudo] password for peter: 
2080+1 records in
2080+1 records out
2181038592 bytes (2.2 GB, 2.0 GiB) copied, 580.6 s, 3.8 MB/s
-----

Since I typically run my boards without any display, you will need
to edit the sshd_config to allow root logins over ssh.
You can do this either by editing this from the pi itself after booting,
or editing the `/etc/ssh/sshd_config` by mounting the sdcard from your
workstation.  Or, set up your own password after logging into the board.
Add:

-----
PermitRootLogin yes
PermitEmptyPasswords yes
-----

There are other ways to change the root password scheme, but these edits
should make logging in via ssh reliable from the start.

Lastly, we can create a standalone toolchain that can be used to
build other software for the board, without needing to adapt the
package for buildroot.

-----
$ make sdk
-----

You should now be able to use the sdcard in the Raspberry Pi, compile
a new binary, scp it to the board and run it.

=== Step 2: Tensorflow Lite

There are many tutorials and scripts to get Tensorflow and Tensorflow Lite
running on different platforms.  With the right invocation, these
tools can be used to build Tensorflow Lite examples to run on our
buildroot image.

First, checkout tensorflow lite from github.

-----
$ git clone https://github.com/tensorflow/tensorflow.git tf-git
$ cd tf-git
$ ./tensorflow/lite/tools/make/download_dependencies.sh # Download additional dependencies
-----

If you use the musl-based toolchain, you will need to hide tensorflow's use of `mallinfo`.  Apply the following patch:

-----
--- a/tensorflow/lite/profiling/memory_info.cc
+++ b/tensorflow/lite/profiling/memory_info.cc
@@ -27,7 +27,7 @@ namespace memory {
 const int MemoryUsage::kValueNotSet = 0;
 
 bool MemoryUsage::IsSupported() {
-#ifdef __linux__
+#if defined(__linux__) && !defined(TFLITE_LACKS_MALLINFO)
   return true;
 #endif
   return false;
@@ -35,7 +35,7 @@ bool MemoryUsage::IsSupported() {
 
 MemoryUsage GetMemoryUsage() {
   MemoryUsage result;
-#ifdef __linux__
+#if defined(__linux__) && !defined(TFLITE_LACKS_MALLINFO)
   rusage res;
   if (getrusage(RUSAGE_SELF, &res) == 0) {
     result.max_rss_kb = res.ru_maxrss;
-----

Now, you're ready to build tensorflow lite, and one of the sample programs.

-----
$ make -j 2 CC_PREFIX=/path/to/buildroot-2020.02.4/output/host/bin/ TARGET_TOOLCHAIN_PREFIX=aarch64-kerikeri-linux-musl- TARGET=aarch64 EXTRA_CXXFLAGS=-DTFLITE_LACKS_MALLINFO=1 -f tensorflow/lite/tools/make/Makefile label_image
-----


This will deposit a label_image binary in:

`tensorflow/lite/tools/make/gen/linux_aarch64/bin/label_image`.

Following the directions from the label_image readme, we copy the necessary pieces to the board to run them.  This is snipped from `tensorflow/lite/examples/label_image/README.md`

-----
# On your workstation:
mkdir -p /tmp/mobilenet
# Get model
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz | tar xzv -C /tmp/mobilenet
# Get labels
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz  | tar xzv -C /tmp/mobilenet  mobilenet_v1_1.0_224/labels.txt

# Copy all of the relevent stuff to your device:
scp /tmp/mobilenet/mobilenet_v1_1.0_224.tflite root@192.168.254.202:~/
scp /tmp/mobilenet/mobilenet_v1_1.0_224/labels.txt root@192.168.254.202:~/
scp tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp root@192.168.254.202:~/
scp tensorflow/lite/tools/make/gen/linux_aarch64/bin/label_image root@192.168.254.202:~/

# Now log into the board, and run the example!
$ ssh root@192.168.254.202
-----

When logged into the board:

-----
# ls
grace_hopper.bmp             labels.txt
label_image                  mobilenet_v1_1.0_224.tflite
./label_image --image grace_hopper.bmp --labels labels.txt --tflite_model mobilenet_v1_1.0_224.tflite
Loaded model mobilenet_v1_1.0_224.tflite
resolved reporter
invoked 
average time: 823.582 ms 
0.860174: 653 653:military uniform
0.0481021: 907 907:Windsor tie
0.00786703: 466 466:bulletproof vest
0.00644931: 514 514:cornet, horn, trumpet, trump
0.00608027: 543 543:drumstick
-----

Next time, we'll explore some ways to optimize the performance of the inference.

